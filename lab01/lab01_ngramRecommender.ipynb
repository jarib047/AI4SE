{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc8d242-d613-4ba1-957e-d896d8c55103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import ast\n",
    "import javalang\n",
    "import json\n",
    "\n",
    "def tokenize_java(code):\n",
    "    \"\"\"\n",
    "    Tokenizes a Java code string using javalang.\n",
    "    Returns a list of tokens as strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(code))\n",
    "        return [t.value for t in tokens]\n",
    "    except Exception as E:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b628f1-3b42-44a9-ae5d-a6dc381836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.model = defaultdict(lambda: defaultdict(int))  # counts\n",
    "        self.prefix_totals = defaultdict(int)              # total counts per prefix\n",
    "        self.vocab = set()                                 # store vocab once\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, tokens_list):\n",
    "        \"\"\"\n",
    "        Build the n-gram counts from a list of tokenized sequences.\n",
    "        \"\"\"\n",
    "        for tokens in tokens_list:\n",
    "            if len(tokens) < self.N:\n",
    "                continue\n",
    "            for i in range(len(tokens) - self.N + 1):\n",
    "                prefix = tuple(tokens[i:i + self.N - 1])\n",
    "                next_token = tokens[i + self.N - 1]\n",
    "                self.model[prefix][next_token] += 1\n",
    "                self.prefix_totals[prefix] += 1\n",
    "                self.vocab.add(next_token)  # update vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        Probability with Laplace smoothing.\n",
    "        O(1) lookup.\n",
    "        \"\"\"\n",
    "        prefix = tuple(prefix)\n",
    "        count_next = self.model[prefix].get(next_token, 0)\n",
    "        total = self.prefix_totals.get(prefix, 0)\n",
    "        return (count_next + 1) / (total + self.vocab_size)\n",
    "\n",
    "    def perplexity(self, tokens_list):\n",
    "        \"\"\"\n",
    "        Compute perplexity over a list of tokenized sequences.\n",
    "        Optimized to avoid recomputing vocab or totals.\n",
    "        \"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for tokens in tokens_list:\n",
    "            if len(tokens) < self.N:\n",
    "                continue\n",
    "            for i in range(len(tokens) - self.N + 1):\n",
    "                prefix = tokens[i:i + self.N - 1]\n",
    "                target = tokens[i + self.N - 1]\n",
    "                count_next = self.model[tuple(prefix)].get(target, 0)\n",
    "                total = self.prefix_totals.get(tuple(prefix), 0)\n",
    "                prob = (count_next + 1) / (total + self.vocab_size)\n",
    "                total_log_prob += math.log(prob)\n",
    "                total_tokens += 1\n",
    "\n",
    "        return math.exp(-total_log_prob / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    def sample_next(self, prefix):\n",
    "        \"\"\"\n",
    "        Sample next token given a prefix.\n",
    "        \"\"\"\n",
    "        prefix = tuple(prefix)\n",
    "        if prefix not in self.model:\n",
    "            return None\n",
    "        tokens = list(self.model[prefix].keys())\n",
    "        counts = list(self.model[prefix].values())\n",
    "        return random.choices(tokens, weights=counts, k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37c8598-5dcc-4870-92ac-f69427b53451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(data_path):\n",
    "    # Load your dataset\n",
    "    # df = pd.read_csv(\"methods.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Convert stringified lists back into Python lists\n",
    "    df[\"new_code\"] = df[\"original_code\"].apply(tokenize_java)\n",
    "    df[\"tokenized_code\"] = df[\"new_code\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    train_tokens = df[df[\"dataset_split\"] == \"train\"][\"tokenized_code\"].tolist()\n",
    "    test_tokens_int  = df[df[\"dataset_split\"] == \"test\"][\"tokenized_code\"].tolist()\n",
    "    test_tokens = test_tokens_int[:4000]\n",
    "    sample_tokens = test_tokens_int[4001:]\n",
    "       \n",
    "    # Try different N values\n",
    "    best_N, best_ppl = None, float(\"inf\")\n",
    "    \n",
    "    for N in range(3, 12, 2):\n",
    "        model = NGramModel(N)\n",
    "        model.train(train_tokens)\n",
    "        ppl = model.perplexity(test_tokens)\n",
    "        print(f\"N={N}, Perplexity={ppl:.2f}\")\n",
    "        if ppl < best_ppl:\n",
    "            best_ppl, best_N = ppl, N\n",
    "    \n",
    "    print(f\"Best model: N={best_N} with perplexity {best_ppl:.2f}\")\n",
    "    return best_N, train_tokens, sample_tokens\n",
    "\n",
    "def sample_sequence(best_N, train_tokens, start_tokens, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate sequence until model can't predict or max_len is reached.\n",
    "    Uses the best N-gram model.\n",
    "    \"\"\"\n",
    "    model = NGramModel(best_N)\n",
    "    model.train(train_tokens)\n",
    "    tok_seq = start_tokens[:]\n",
    "    result = start_tokens[:]\n",
    "    while len(tok_seq) < max_len:\n",
    "        prefix = tok_seq[-(best_N- 1):]  # last N-1 tokens\n",
    "        next_tok = model.sample_next(prefix)\n",
    "        probab = round(model.prob(prefix, next_tok), 5)\n",
    "        if next_tok is None:\n",
    "            break  # no continuation found\n",
    "        result.append((next_tok, probab))\n",
    "        tok_seq.append(next_tok)\n",
    "    return tok_seq, result\n",
    "\n",
    "\n",
    "def write_output(op_data, op_filename = \"output.json\"):\n",
    "    with open(op_filename, \"w\") as f:\n",
    "        json.dump(op_data, f, indent=4) # indent for pretty-printing\n",
    "    print(f\"Output successfully written to {op_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "142b2049-82b4-4552-a749-99a6a4d7613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_path = \"methods.csv\"\n",
    "    best_N, train_tokens, sample_tokens = find_best_model(data_path)\n",
    "    sample_tokens = [i[:best_N] for i in sample_tokens]\n",
    "    result_dict = {}\n",
    "    for i, seq in enumerate(sample_tokens):\n",
    "        generated, prob_seq = sample_sequence(best_N, train_tokens, seq, max_len=30)\n",
    "        # print(\"Input:\", seq)\n",
    "        # print(\"Generated token:\", generated)\n",
    "        # print(\"Token probability:\", prob_seq)\n",
    "        # print()\n",
    "        result_dict[i] = {\"input\":seq,\n",
    "                          \"generated tokens\": generated,\n",
    "                          \"token prob\": prob_seq}\n",
    "    write_output(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b72d833d-d864-4a1e-bb8f-3f7dde4a399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=3, Perplexity=2709.65\n",
      "N=5, Perplexity=15135.37\n",
      "N=7, Perplexity=30042.56\n",
      "N=9, Perplexity=38290.17\n",
      "N=11, Perplexity=42397.12\n",
      "Best model: N=3 with perplexity 2709.65\n",
      "Output successfully written to output.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884c040-13d3-4b64-9e81-a4042fd47dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b0ef7-9bb5-4185-bd3e-93756e5dcfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfa7c0-b10e-46e5-b17a-2f305d465a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182abfee-985e-49a5-9122-23cc18901945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5442f-f7cb-43b4-bde5-a617e2396ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e72632-3227-4a6a-a75a-589726854762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231aa885-bd94-4ad0-8760-cfc49bb27597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43299dfc-40aa-4629-9e15-50606d95bd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
